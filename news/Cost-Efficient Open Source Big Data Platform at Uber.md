
> Source : https://eng.uber.com/cost-efficient-big-data-platform/

## 개요

우버의 비즈니스가 확장됨에 따라 우버의 데이터 풀이 기하급수적으로 증가하여 처리 비용이 높아졌다.  
우리는 데이터 플랫폼의 비용을 줄이기 위한 작업을 시작했으며, 이는 플랫폼 효율성, 공급 및 수요의 3가지 주제로 문제를 나뉘었다.  
이 글에서 우리는 데이터 플랫폼의 효율성을 개선하고 비용을 낮추기 위한 우리의 노력에 대해서 말할 것이다

## Big Data File Format Optimizations

우리의 HDFS는 Hive 테이블로 대부분 채워져 있다.  
이러한 테이블은 Parquet, ORC 파일 포맷으로 저장되어 있다.  
두 포맷 모두 블록 기반의 columnar 포맷이다. 즉, 파일은 각 컬럼별로 여러 블럭들로 구성되어있다 (각 블록은 많은 행으로 구성되어 있음 대략 10,000개 씩)  
우리는 HDFS 파일에 대해 상당한 시간을 조사하였고 Parquet 형식에 중점을 두어 최적화를 수행하기로 결정하였다  

1. 압축 알고리즘
   1. 기본적으로 Parquet 내부의 압축 알고리즘으로 GZIP 6레벨을 사용한다. Facebook에서는 최근 ZSTD 압축 방식을 Parquet 포맷에 대해 실험한 적이 있는데 이러한 실험이 우리에게 큰 인상을 남겼다.  
   2. 그들의 실험에서 ZSTD 레벨 9 및 레벨 19는 GZIP 기반 Parquet 파일에 비해 파일 크기를 각각 8%, 12% 줄일 수 있었다. 또한, 압축 해제 속도도 더 빠르다.
   3. 우리는 1개월 후에 데이터를 재압축할 때 ZSTD 레벨 9를, 3개월 후에 ZSTD 레벨 19를 사용하기로 결정했다. ZSTD 레벨 9 압축이 레벨 19보다 3배 빠르기 때문이다.
2. 컬럼 삭제
   1. 많은 Hive 테이블에는 수 많은 열이 포함되어 있으며 그 중 일부는 중첩되어 있다.
   2. 이러한 열을 살펴보면 그 중 일부분은 장기간 보관할 필요가 없다는 것을 알 수 있다.
   3. 열 포맷이 주어졌을 때 열의 압축 해제 및 재압축 없이 파일 내부적으로 열을 삭제하는 것이 가능하다. 이것은 컬럼 삭제를 CPU 효율적인 작업으로 만든다.
   4. 우버에서 이러한 기능을 구현하고 Hive 테이블에 광범위하게 사용하고 있다.
3. 행 정렬
   1. 행 순서는 Parquet 압축 파일 크기에 큰 영향을 줄 수 있다. 이는 Parquet 내부의 Run-Length Encoding 기능과 로컬 반복을 활용하는 압축 알고리즘의 기능 때문이다
   2. 우버에서 가장 큰 Hive 테이블을 대상으로 순서를 수동으로 조정하였을 때 테이블 크기가 50% 이상 줄어들었다
   3. 행을 정렬하는 단순한 방법 중 하나는 로그 테이블에 대한 타임스탬프를 ID별로 지정하는 것이다
   4. 대부분의 로그 테이블은 사용자 ID와 타임스탬프 컬럼이 있기 때문에 이를 통해 컬럼을 매우 잘 압축할 수 있다
4. 델타 인코딩
   1. 타임스태프별로 행을 정렬하게 되면 타임스탬프 간의 차이를 표현하는 크기가 타임스탬프 값 자체를 표현하는 것보다 매우 작기 때문에 Delta Encoding이 데이터 크기를 더 줄일 수 있다고 생각하였다.
   2. 어떤 경우에는 로그가 하트 비트와 같이 일정한 리듬을 가지고 있어서 그 차이가 일정하다
   3. 하지만 Hive, Presto, Spark가 널리 사용되는 환경에서는 [StackOverflow](https://stackoverflow.com/questions/60808914/write-a-parquet-file-with-delta-encoded-coulmns) 질문에 언급된 것처럼 Parquet에 Delta Encoding을 적용하기가 쉽지 않다. 하지만 계속 방법을 모색 중이다.

## HDFS Erasure Coding

Erasure 코딩은 HDFS 파일의 복제 계수를 크게 줄일 수 있다.  
잠재적으로 증가할 수 있는 IOPS 워크로드로 인해 우버에서는 복제 계수가 1.67 및 1.5를 보여주고 있다. (3+2 및 6+3 스키마가 무슨 말..?)  
기본 복제 계수가 3이라는 것을 감안할 때 필요한 디스크 공간을 거의 절반으로 줄일 수 있다

Erasure 코딩을 적용할 수 있는 방법은 다양하다
1. Apache Hadoop 3.0 HDFS Erasure Code
   1. 하둡 3.0에서 공식적으로 구현된 Erasure Code이다. 이 구현의 좋은 점은 크고 작은 파일 모두에서 동작한다는 것이지만 단점은 블록이 Erasure Code에 대해 매두 단편화되기 때문에 IO 효율성이 좋지 않다
2. Client-side Erasure Code
   1. HDFS-RAID 프로젝트에서 페이스북에 의해 처음 구현된 방식이다.
   2. 이 방식의 좋은 점은 매우 IO 효율적이라는 것이고 단점은 작은 파일에는 작동하지 않는다는 것이다.

많은 상의 끝에 우리는 첫 번째 방식인 Apache Hadoop 3.0 HDFS Erasure Code 를 사용하기로 결정했다.
아직은 우리도 테스트 단계에 있지만 이 것을 적용했을 때 HDFS 비용을 줄이는데 큰 영향을 줄 것이라고 확신하고 있다

## YARN Scheduling Policy Improvements

우버에서는 Yarn을 통해서 대부분의 빅데이터 연산을 실행한다.  
다른 대부분의 회사와 마찬가지로 표준 Capacity 스케줄러 방식을 사용했다.  
큐를 계층적 구조로 구성하였으며 MIN 및 MAX 설정을 적용하였다.

이러한 스케줄링 방식을 사용했을 때 처음에는 좋았지만 어느 순간 클러스터 용량 관리에 대한 딜레마를 제공하게 되었다.

1. High Utilization
   1. 우리는 YARN 클러스터의 평균 사용률을 가능한 한 높게 유지하고 싶다
2. Meeting User Expectations
   1. 우리는 클러스터 자원을 사용하는 사용자들에게 그들이 기대하는 리소스 양을 제공하고 싶다

